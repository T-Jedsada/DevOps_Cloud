{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytest_working.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g60m0M2zS1Tn",
        "colab_type": "text"
      },
      "source": [
        "# Install Pytest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGynl4A8SjFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install pytest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjvw049yS8fQ",
        "colab_type": "text"
      },
      "source": [
        "# Run the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKe1eHDvS0sa",
        "colab_type": "code",
        "outputId": "0f331d4b-d3cd-426c-9eda-48640bf8389f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pytest /content/test_getting_started.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_getting_started.py F\u001b[36m                                                [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m_________________________________ test_answer __________________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_answer():\u001b[0m\n",
            "\u001b[1m>   \tassert func(3) == 8\u001b[0m\n",
            "\u001b[1m\u001b[31mE    assert 4 == 8\u001b[0m\n",
            "\u001b[1m\u001b[31mE     +  where 4 = func(3)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_getting_started.py\u001b[0m:5: AssertionError\n",
            "\u001b[1m\u001b[31m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks_utFPxUJM-",
        "colab_type": "text"
      },
      "source": [
        "# Run multiple tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNrfoeS4Uag9",
        "colab_type": "text"
      },
      "source": [
        "pytest will run all files of the form test_*.py or *_test.py in the current directory and its subdirectories.   Moregenerally, it followsstandard test discovery rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlSPQAheTXsu",
        "colab_type": "code",
        "outputId": "a5956797-ba2f-446a-e549-da30fa8d92a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "!pytest /content/test_raising_exception.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_raising_exception.py .\u001b[36m                                              [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUNDAU-fVXhb",
        "colab_type": "text"
      },
      "source": [
        "# Group Multiple tests in a class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8PidqBNVq4s",
        "colab_type": "text"
      },
      "source": [
        "pytestdiscovers  all  tests  following  itsConventions  for  Python  test  discovery,  so  it  finds  bothtest_prefixedfunctions.  There is no need to subclass anything, but make sure to prefix your class withTestotherwise the classwill be skipped. We can simply run the module by passing its filename:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbHfy0dTVRzH",
        "colab_type": "code",
        "outputId": "c26c2e94-b515-41c3-e5cc-0f08114b67c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!pytest -q /content/test_group.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".F\u001b[36m                                                                       [100%]\u001b[0m\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m______________________________ TestClass.test_two ______________________________\u001b[0m\n",
            "\n",
            "self = <test_group.TestClass instance at 0x7f680d6753c0>\n",
            "\n",
            "\u001b[1m    def test_two(self):\u001b[0m\n",
            "\u001b[1m            x = \"hello\"\u001b[0m\n",
            "\u001b[1m>           assert hasattr(x, \"check\")\u001b[0m\n",
            "\u001b[1m\u001b[31mE           AssertionError: assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE            +  where False = hasattr('hello', 'check')\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_group.py\u001b[0m:8: AssertionError\n",
            "\u001b[1m\u001b[31m1 failed, 1 passed in 0.03 seconds\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBt_iDbEV9BO",
        "colab_type": "text"
      },
      "source": [
        "The first test passed and the second failed.  You can easily see the intermediate values in the assertion to help youunderstand the reason for the failure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBMBULBDWi6e",
        "colab_type": "text"
      },
      "source": [
        "# Temp Directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcQLmziUWonW",
        "colab_type": "text"
      },
      "source": [
        "pytestprovides Builtin fixtures/function arguments to request arbitrary resources, like a unique temporary directory:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNjeW6iPWr1R",
        "colab_type": "text"
      },
      "source": [
        "List the nametmpdirin the test function signature andpytestwill lookup and call a fixture factory to create theresource before performing the test function call.  Before the test runs,pytestcreates a unique-per-test-invocationtemporary directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAGSVij2V1xD",
        "colab_type": "code",
        "outputId": "7f0f0feb-ebef-4170-be59-b966eed97945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "!pytest /content/test_temp_dir.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_temp_dir.py .\u001b[36m                                                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps7xCB9kWzL5",
        "colab_type": "text"
      },
      "source": [
        "# Possible exit codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oDY8wtZW2Gq",
        "colab_type": "text"
      },
      "source": [
        "Runningpytestcan result in six different exit codes:\n",
        "\n",
        "Exit code 0  All tests were collected and passed successfully\n",
        "\n",
        "Exit code 1  Tests were collected and run but some of the tests failed\n",
        "\n",
        "Exit code 2  Test execution was interrupted by the user\n",
        "\n",
        "Exit code 3  Internal error happened while executing tests\n",
        "\n",
        "Exit code 4  pytest command line usage error\n",
        "\n",
        "Exit code 5  No tests were collectedThey are represented by the_pytest.config.ExitCodeenum.  The exit codes being a part of the public APIcan be imported and accessed directly using:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9g5p4ggWuNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytest import ExitCode "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g17MLuszXPsx",
        "colab_type": "text"
      },
      "source": [
        "To stop the testing process after the first (N) failures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDx4_un0XEEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pytest -x               # stop after first failure\n",
        "!pytest --maxfail=2      # stop after two failures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPtb-NiLXYKi",
        "colab_type": "text"
      },
      "source": [
        "# Run tests in a module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63CYeYrEXZj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pytest test_group.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdGGFVsDXcka",
        "colab_type": "text"
      },
      "source": [
        "# Run tests in a directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvoEDhz-Xboc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pytest /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9W-QITiXnLy",
        "colab_type": "text"
      },
      "source": [
        "# Run tests by keyword expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txqbTj24Xh8L",
        "colab_type": "code",
        "outputId": "262c5b2e-fed5-47ac-e4f4-cc0e6b513b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "!pytest -k \"f\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollecting 5 items                                                             \u001b[0m\u001b[1m\rcollected 5 items / 4 deselected                                               \u001b[0m\n",
            "\n",
            "test_temp_dir.py .\u001b[36m                                                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m==================== 1 passed, 4 deselected in 0.02 seconds ====================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3s1unuLXr-I",
        "colab_type": "text"
      },
      "source": [
        "This  will  run  tests  which  contain  names  that  match  the  givenstring  expression(case-insensitive),  which  can  in-clude Python operators that use filenames,  class names and function names as variables.   \n",
        "\n",
        "The example above will runTestMyClass.test_something but not TestMyClass.test_method_simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXsq_mXPYQZT",
        "colab_type": "text"
      },
      "source": [
        "# Modifying Python traceback printing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlDJY-ykYVCN",
        "colab_type": "text"
      },
      "source": [
        "pytest --showlocals # show local variables in tracebacks \n",
        "\n",
        "pytest -l # show local variables (shortcut)\n",
        "\n",
        "pytest --tb=auto  # (default) 'long' tracebacks for the first and last# entry, but 'short' style for the other entries \n",
        "\n",
        "pytest --tb=long # exhaustive, informative traceback formatting\n",
        "\n",
        "pytest --tb=short # shorter traceback format\n",
        "\n",
        "pytest --tb=line # only one line per failure\n",
        "\n",
        "pytest --tb=native# Python standard library formatting\n",
        "\n",
        "pytest --tb=no# no traceback at all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCI7ZT9AYoMl",
        "colab_type": "text"
      },
      "source": [
        "# Detailed summary report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCYbccAgYzRj",
        "colab_type": "text"
      },
      "source": [
        "The -r flag can be used to display a “short test summary info” at the end of the test session, making it easy in largetest suites to get a clear picture of all failures, skips, xfails, etc.\n",
        "\n",
        "It defaults tofEto list failures and errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgVJY-rGXsJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pytest -ra /content/test_summary.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvuqEsjxaVyO",
        "colab_type": "text"
      },
      "source": [
        "The-roptions accepts a number of characters after it, withaused above meaning “all except passes”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbN4O2j8aW7u",
        "colab_type": "text"
      },
      "source": [
        "f failed\n",
        "\n",
        "e error\n",
        "\n",
        "s skipped\n",
        "\n",
        "x xfailed\n",
        "\n",
        "X xpassed\n",
        "\n",
        "p passed\n",
        "\n",
        "P passed with output\n",
        "\n",
        "a all except pP\n",
        "\n",
        "A all\n",
        "N none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPe0ffbhZmnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pytest -rfs /content/test_summary.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvk-iHbKcSYm",
        "colab_type": "text"
      },
      "source": [
        "# Calling pytest from Python code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHtZwR7GcXbq",
        "colab_type": "text"
      },
      "source": [
        "You can invokepytestfrom Python code directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKZpeBnBaqXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pytest.main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVYI_l0McbXi",
        "colab_type": "text"
      },
      "source": [
        "this acts as if you would call “pytest” from the command line. It will not raiseSystemExitbut return the exitcodeinstead. You can pass in options and arguments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26hfbme5cbsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pytest.main([\"-x\", \"mytestdir\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEfYzBbKcjeW",
        "colab_type": "text"
      },
      "source": [
        "You can specify additional plugins to pytest.main:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgrVSsvkpCb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python myinvoke.py"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}